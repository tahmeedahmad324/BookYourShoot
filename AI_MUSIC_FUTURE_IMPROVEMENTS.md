# AI Music Module - Future Improvements Guide
## How to Achieve 99% Accuracy Through Fine-Tuning

---

## Current Status (January 2026)

**Current Approach:** Zero-shot CLIP classification + YOLO person detection
- **Accuracy:** ~60-70% (varies by event type)
- **Pros:** Works immediately, no training data needed
- **Cons:** Generic prompts don't capture Pakistani wedding specifics well
- **Person Detection:** Filters out gaming screenshots, landscapes automatically

**NEW FEATURE (February 2026):** üéµ **Multi-Image Aggregate Music Suggestion**
- Upload 2-20 event images at once
- AI analyzes each image independently
- Confidence-weighted voting determines final event type
- More images = higher accuracy (aggregation reduces noise)
- See "Multi-Image Feature" section below for details

---

## Data Augmentation: When to Use It vs. Zero-Shot

### ‚ö†Ô∏è Important Decision Guide

**If you have VERY FEW PHOTOS (<30 per event):**
- ‚úÖ **STICK WITH ZERO-SHOT** - Fine-tuning will cause overfitting
- ‚ùå **DON'T TRAIN** - Model will memorize, not generalize
- üîß **Improve prompts instead** - Better prompts = better zero-shot accuracy

**If you have 30-50 photos per event WITH augmentation:**
- ‚ö†Ô∏è **RISKY BUT POSSIBLE** - Use aggressive augmentation (see below)
- üìä **Expected improvement:** 5-15% accuracy gain
- üéØ **Trade-off:** Risk of overfitting vs. potential improvement

**If you have 50-100+ photos per event:**
- ‚úÖ **FINE-TUNE RECOMMENDED** - Sweet spot for training
- üìä **Expected improvement:** 20-35% accuracy gain
- üöÄ **Best approach:** Combine fine-tuning + augmentation

### Smart Augmentation Strategy (30-50 photos)

If you decide to train with limited data:

```python
# Add to training script: backend/scripts/finetune_clip.py

from torchvision import transforms
import torch

# AGGRESSIVE augmentation to expand limited dataset
augmentation_pipeline = transforms.Compose([
    # Geometric transformations
    transforms.RandomHorizontalFlip(p=0.5),  # Mirror images
    transforms.RandomRotation(degrees=15),    # Slight rotation
    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),  # Zoom in/out
    
    # Color/lighting transformations (critical for events with varied lighting)
    transforms.ColorJitter(
        brightness=0.3,   # ¬±30% brightness (indoor/outdoor variance)
        contrast=0.3,     # ¬±30% contrast (flash photography)
        saturation=0.3,   # ¬±30% saturation (colorful decorations)
        hue=0.1           # Slight hue shift
    ),
    
    # Perspective and noise
    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),
    transforms.RandomGrayscale(p=0.05),  # 5% chance to grayscale
    
    # Normalization (required for CLIP)
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Apply 5x augmentation: each real photo generates 5 variants
# 30 real photos ‚Üí 150 training samples
# 50 real photos ‚Üí 250 training samples
```

**Augmentation Ratios by Dataset Size:**
- **20-30 photos**: 8x augmentation (160-240 samples) - HIGH RISK
- **30-50 photos**: 5x augmentation (150-250 samples) - MODERATE RISK
- **50-100 photos**: 3x augmentation (150-300 samples) - LOW RISK
- **100+ photos**: 2x augmentation (200+ samples) - SAFE

### Validation Strategy (Critical for Limited Data)

```python
from sklearn.model_selection import StratifiedKFold

# Use 5-fold cross-validation instead of simple train/test split
# This ensures every photo is used for both training and validation
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(kfold.split(images, labels)):
    print(f"\nüîÑ Training Fold {fold + 1}/5")
    
    # Train on 80% of data
    train_subset = torch.utils.data.Subset(dataset, train_idx)
    
    # Validate on remaining 20%
    val_subset = torch.utils.data.Subset(dataset, val_idx)
    
    # Train model...
    # Track validation accuracy per fold
```

**Stop training if validation accuracy drops** - sign of overfitting!

---

---

## üéµ Multi-Image Aggregate Music Suggestion (NEW - February 2026)

### Feature Overview

**Problem Solved:** Single image analysis can be unreliable due to:
- Ambiguous photos (e.g., portrait could be mehndi or walima)
- Poor lighting or framing
- Non-representative shots

**Solution:** Upload multiple images ‚Üí AI aggregates predictions ‚Üí Higher confidence!

### How It Works

```
User uploads 10 mehndi photos
    ‚Üì
Image 1: mehndi (85%) ‚Üí +0.85 votes
Image 2: mehndi (92%) ‚Üí +0.92 votes
Image 3: barat (45%)  ‚Üí +0.45 votes (misclassified)
Image 4: mehndi (88%) ‚Üí +0.88 votes
... (6 more images)
    ‚Üì
Aggregate: mehndi = 8.7, barat = 0.8, walima = 0.5
    ‚Üì
Winner: MEHNDI (87% average confidence)
    ‚Üì
üéµ Suggest mehndi-specific music!
```

### API Endpoint

**POST** `/api/music/suggest-from-images`

**Request:** Multipart form-data with 2-20 images

```bash
curl -X POST http://localhost:8000/api/music/suggest-from-images \
  -F "images=@photo1.jpg" \
  -F "images=@photo2.jpg" \
  -F "images=@photo3.jpg"
```

**Response:**
```json
{
  "success": true,
  "analysis": {
    "total_images_uploaded": 10,
    "successfully_analyzed": 10,
    "aggregate_event_type": "mehndi",
    "aggregate_confidence": 0.87,
    "confidence_percentage": "87.0%",
    "all_event_votes": {
      "mehndi": 0.87,
      "barat": 0.08,
      "walima": 0.05
    },
    "individual_predictions": [
      {
        "image_index": 1,
        "filename": "photo1.jpg",
        "detected_event": "mehndi",
        "confidence": 0.85,
        "all_scores": {"mehndi": 0.85, "barat": 0.10, ...}
      },
      // ... 9 more predictions
    ]
  },
  "music_suggestions": {
    "event_type": "mehndi",
    "total_tracks": 45,
    "tracks": [
      {
        "title": "Dholki Bajegi",
        "artist": "Naseebo Lal",
        "spotifyUrl": "...",
        // ... track details
      },
      // ... 29 more tracks
    ]
  },
  "message": "Detected 'mehndi' event with 87.0% confidence. Found 45 matching tracks."
}
```

### Frontend Integration Example

```javascript
// React component for multi-image upload

import React, { useState } from 'react';
import axios from 'axios';

function MultiImageMusicSuggestion() {
  const [selectedImages, setSelectedImages] = useState([]);
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);

  const handleImageSelect = (event) => {
    const files = Array.from(event.target.files).slice(0, 20);
    setSelectedImages(files);
  };

  const handleSubmit = async () => {
    if (selectedImages.length === 0) {
      alert('Please select at least 1 image');
      return;
    }

    setLoading(true);
    const formData = new FormData();
    
    selectedImages.forEach(file => {
      formData.append('images', file);
    });

    try {
      const response = await axios.post(
        'http://localhost:8000/api/music/suggest-from-images',
        formData,
        {
          headers: { 'Content-Type': 'multipart/form-data' }
        }
      );

      setAnalysis(response.data);
    } catch (error) {
      console.error('Error:', error);
      alert('Failed to analyze images');
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="multi-image-music">
      <h2>üéµ AI Music Suggestion from Photos</h2>
      
      <input
        type="file"
        multiple
        accept="image/*"
        onChange={handleImageSelect}
        max="20"
      />
      
      <p>Selected: {selectedImages.length} images (max 20)</p>
      
      <button onClick={handleSubmit} disabled={loading}>
        {loading ? 'Analyzing...' : 'Get Music Suggestions'}
      </button>

      {analysis && (
        <div className="results">
          <h3>üìä Analysis Results</h3>
          <p><strong>Event Type:</strong> {analysis.analysis.aggregate_event_type}</p>
          <p><strong>Confidence:</strong> {analysis.analysis.confidence_percentage}</p>
          
          <h3>üéµ Suggested Music ({analysis.music_suggestions.total_tracks} tracks)</h3>
          <ul>
            {analysis.music_suggestions.tracks.map((track, idx) => (
              <li key={idx}>
                {track.title} - {track.artist}
                <a href={track.spotifyUrl} target="_blank">Play on Spotify</a>
              </li>
            ))}
          </ul>
        </div>
      )}
    </div>
  );
}

export default MultiImageMusicSuggestion;
```

### Advantages Over Single-Image Analysis

| Metric | Single Image | Multi-Image (10 photos) |
|--------|--------------|-------------------------|
| Accuracy | 60-70% | **75-85%** ‚úÖ |
| Confidence | Varies wildly | Stable average |
| Robustness | Fails on ambiguous photos | Self-correcting |
| User Trust | Low ("just one photo?") | High ("AI saw whole event") |

### Recommendations for Users

**Optimal Number of Images:**
- **Minimum:** 2 images (better than 1)
- **Sweet Spot:** 5-10 images (best accuracy/upload time balance)
- **Maximum:** 20 images (diminishing returns after that)

**Photo Selection Tips:**
- Mix wide shots and close-ups
- Include decorations, attire, and people
- Different moments from the event
- Avoid blurry or dark photos

### Technical Details

**Processing Time:**
- ~2-3 seconds per image (CLIP inference)
- 10 images = ~25-30 seconds total
- Runs on CPU (no GPU required)

**Memory Usage:**
- ~100MB per image in memory
- 20 images max = ~2GB peak RAM usage

**Fallback Behavior:**
- If some images fail to analyze, others compensate
- Requires at least 1 successful analysis
- Returns individual errors for transparency

---

## Path to 99% Accuracy: Fine-Tuning on Your Data (Updated)

### Step 1: Collect Training Data (Required)

Gather **50-100 photos per event type**:

```
training_data/
‚îú‚îÄ‚îÄ mehndi/
‚îÇ   ‚îú‚îÄ‚îÄ photo1.jpg
‚îÇ   ‚îú‚îÄ‚îÄ photo2.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ... (50-100 images)
‚îú‚îÄ‚îÄ barat/
‚îÇ   ‚îú‚îÄ‚îÄ photo1.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ... (50-100 images)
‚îú‚îÄ‚îÄ walima/
‚îÇ   ‚îî‚îÄ‚îÄ ... (50-100 images)
‚îú‚îÄ‚îÄ birthday/
‚îÇ   ‚îî‚îÄ‚îÄ ... (50-100 images)
‚îî‚îÄ‚îÄ corporate/
    ‚îî‚îÄ‚îÄ ... (50-100 images)
```

**Quality Guidelines:**
- Use real photos from actual events (ask photographers to share)
- Include variety: different venues, lighting, angles
- Label correctly: ensure each folder contains only that event type
- Mix formal portraits and candid shots
- Include both indoor and outdoor events

---

### Step 2: Choose Fine-Tuning Approach

#### Option A: Fine-tune CLIP (Recommended) ‚≠ê
**Best for:** Maintaining CLIP's understanding + adding Pakistani event knowledge

**Process:**
1. Keep CLIP's pre-trained vision encoder frozen
2. Fine-tune only the projection layers
3. Train on your labeled event photos
4. Use contrastive learning with Pakistani event descriptions

**Expected Accuracy:** 85-95%
**Training Time:** 1-2 hours on GPU / 4-6 hours on CPU
**Code:** Update `clip_analysis_service.py` with fine-tuning script

#### Option B: Train Custom CNN (ResNet/EfficientNet)
**Best for:** Faster inference, smaller model

**Process:**
1. Use pre-trained ResNet50 or EfficientNet-B0
2. Replace final layer with 5-class output (mehndi, barat, walima, birthday, corporate)
3. Fine-tune on your data with data augmentation

**Expected Accuracy:** 90-98%
**Training Time:** 30 min - 1 hour
**Code:** Create new `custom_classifier_service.py`

#### Option C: Vision Transformer (ViT)
**Best for:** Maximum accuracy

**Process:**
1. Use `google/vit-base-patch16-224-in21k`
2. Fine-tune on your event photos
3. Heavier model but best accuracy

**Expected Accuracy:** 95-99%
**Training Time:** 2-3 hours on GPU

---

### Step 3: Implementation Steps

#### Phase 1: Data Preparation (1 day)
```python
# Script: backend/scripts/prepare_training_data.py

from pathlib import Path
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader

class EventDataset(Dataset):
    """Load event photos for training"""
    def __init__(self, data_dir, transform=None):
        self.data_dir = Path(data_dir)
        self.images = []
        self.labels = []
        
        # Event type to numeric label
        self.event_to_label = {
            'mehndi': 0, 'barat': 1, 'walima': 2, 
            'birthday': 3, 'corporate': 4
        }
        
        # Load all images
        for event_type, label_id in self.event_to_label.items():
            event_dir = self.data_dir / event_type
            if event_dir.exists():
                for img_path in event_dir.glob('*.jpg'):
                    self.images.append(img_path)
                    self.labels.append(label_id)
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        image = Image.open(self.images[idx]).convert('RGB')
        label = self.labels[idx]
        
        if self.transform:
            image = self.transform(image)
        
        return image, label
```

#### Phase 2: Fine-Tuning Script (2-3 hours)
```python
# Script: backend/scripts/finetune_clip.py

from transformers import CLIPProcessor, CLIPModel
import torch
from torch.optim import AdamW
from tqdm import tqdm

def finetune_clip(data_dir, epochs=10, batch_size=16):
    """Fine-tune CLIP on Pakistani event photos"""
    
    # Load pre-trained CLIP
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    
    # Freeze vision encoder, only train projection layers
    for param in model.vision_model.parameters():
        param.requires_grad = False
    
    # Load dataset
    train_dataset = EventDataset(data_dir)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    # Optimizer
    optimizer = AdamW(model.parameters(), lr=1e-5)
    
    # Training loop
    model.train()
    for epoch in range(epochs):
        for images, labels in tqdm(train_loader):
            # Forward pass
            outputs = model(pixel_values=images)
            
            # Compute loss (contrastive learning)
            loss = compute_contrastive_loss(outputs, labels)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")
    
    # Save fine-tuned model
    model.save_pretrained("backend/models/clip_finetuned")
    processor.save_pretrained("backend/models/clip_finetuned")
    
    print("‚úÖ Fine-tuning complete!")
```

#### Phase 3: Update Service (30 min)
```python
# Update: backend/services/clip_analysis_service.py

def _load_model(self):
    """Load fine-tuned model if available, else use pre-trained"""
    
    # Check if fine-tuned model exists
    finetuned_path = Path("backend/models/clip_finetuned")
    
    if finetuned_path.exists():
        print("üîÑ Loading FINE-TUNED CLIP model...")
        model_name = str(finetuned_path)
    else:
        print("üîÑ Loading pre-trained CLIP model...")
        model_name = "openai/clip-vit-base-patch32"
    
    self.model = CLIPModel.from_pretrained(model_name)
    self.processor = CLIPProcessor.from_pretrained(model_name)
    # ... rest of code
```

---

### Step 4: Testing & Validation

1. **Split your data:** 80% training, 20% validation
2. **Test on validation set:**
   ```python
   from sklearn.metrics import classification_report, confusion_matrix
   
   # Get predictions
   y_true = [...]  # True labels
   y_pred = [...]  # Model predictions
   
   # Evaluate
   print(classification_report(y_true, y_pred))
   print(confusion_matrix(y_true, y_pred))
   ```

3. **Measure accuracy per event type:**
   - Mehndi: target 95%+
   - Barat: target 98%+ (most common)
   - Walima: target 95%+
   - Birthday: target 90%+
   - Corporate: target 85%+

---

## Alternative: API-Based Solutions (If Training is Too Complex)

### Option 1: Google Cloud Vision API
- **Accuracy:** 85-90% (with custom labels)
- **Cost:** $1.50 per 1000 images
- **Setup:** Enable Vision API, train AutoML model

### Option 2: AWS Rekognition Custom Labels
- **Accuracy:** 90-95%
- **Cost:** Pay per training hour + per image
- **Setup:** Upload training data to S3, train model

### Option 3: Azure Custom Vision
- **Accuracy:** 85-92%
- **Cost:** Free tier: 5000 predictions/month
- **Setup:** Upload photos via portal, train model

---

## Recommended Next Steps

### Immediate (No Training Required)
1. ‚úÖ Person detection (YOLO) - Already implemented
2. ‚úÖ Accept all images with people - Already implemented
3. ‚è≥ Collect feedback from users - Track misclassifications
4. ‚è≥ Improve prompts based on common errors

### Short-term (1-2 weeks, when you have data)
1. Collect 50-100 photos per event type from photographers
2. Run fine-tuning script on collected data
3. Deploy fine-tuned model
4. A/B test: compare zero-shot vs fine-tuned accuracy

### Long-term (1-2 months)
1. Build feedback loop: let users correct misclassifications
2. Retrain monthly with new data
3. Add more event types (nikah, mayun, engagement)
4. Multi-label classification (e.g., "formal + romantic")

---

## Expected Results After Fine-Tuning

| Event Type | Zero-Shot (Current) | After Fine-Tuning | Improvement |
|------------|---------------------|-------------------|-------------|
| Mehndi     | ~60%               | **95%**           | +35%        |
| Barat      | ~65%               | **98%**           | +33%        |
| Walima     | ~55%               | **95%**           | +40%        |
| Birthday   | ~70%               | **92%**           | +22%        |
| Corporate  | ~60%               | **88%**           | +28%        |
| **Overall** | **62%**           | **94%**           | **+32%**    |

---

## Contact for Implementation

When ready to implement fine-tuning:
1. Share training data in the folder structure above
2. I'll write the complete fine-tuning script
3. Training time: ~2-3 hours on your machine
4. Deploy updated model (just replace model file)

**Storage Requirements:**
- Training data: ~500MB-1GB (500 images total)
- Fine-tuned model: ~600MB
- Total: ~1.5GB disk space

---

## Notes

- Person detection (YOLO) currently filters out obvious non-events (landscapes, screenshots)
- Fine-tuning CLIP is the best balance of accuracy and implementation complexity
- If you have <30 photos per category, zero-shot is actually better (overfitting risk)
- Consider starting with just mehndi/barat/walima (main wedding events) first

---

Last Updated: January 25, 2026
